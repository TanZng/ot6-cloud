{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://16685f575a69:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>jupyter-notebook-analytics</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f81966a2f10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark session & context\n",
    "spark = SparkSession.builder.master(\"spark://spark:7077\") \\\n",
    "        .appName(\"jupyter-notebook-analytics\") \\\n",
    "        .config(\"spark.driver.memory\", \"512m\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://mongodb:27017/test.myCollection\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://mongodb:27017/test.myCollection\") \\\n",
    "        .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.2') \\\n",
    "        .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://16685f575a69:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>jupyter-notebook-analytics</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://spark:7077 appName=jupyter-notebook-analytics>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum of the first 100 whole numbers\n",
    "from pyspark.rdd import RDD\n",
    "\n",
    "rdd = sc.parallelize(range(1000+1))\n",
    "rdd.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mongo Spark Connector\n",
    "Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+\n",
      "|         name| age|\n",
      "+-------------+----+\n",
      "|Bilbo Baggins|  50|\n",
      "|      Gandalf|1000|\n",
      "|       Thorin| 195|\n",
      "|        Balin| 178|\n",
      "|         Kili|  77|\n",
      "|       Dwalin| 169|\n",
      "|          Oin| 167|\n",
      "|        Gloin| 158|\n",
      "|         Fili|  82|\n",
      "|       Bombur|  50|\n",
      "+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people = spark.createDataFrame([(\"Bilbo Baggins\",  50), (\"Gandalf\", 1000), (\"Thorin\", 195), (\"Balin\", 178), (\"Kili\", 77),\n",
    "   (\"Dwalin\", 169), (\"Oin\", 167), (\"Gloin\", 158), (\"Fili\", 82), (\"Bombur\", 50)], schema='name string, age int')\n",
    "\n",
    "people.write.format(\"mongo\").mode(\"append\").save()\n",
    "# people.write.format(\"mongodb\").mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to write to a different MongoDB collection, use the .option() \n",
    "# method with .write().\n",
    "# To write to a collection called contacts in a database called people, \n",
    "# specify the collection and database with .option():\n",
    "# OLD: people.write.format(\"mongodb\").mode(\"append\").option(\"database\",\"people\").option(\"collection\", \"contacts\").save()\n",
    "# people.write.format(\"mongo\").mode(\"append\").option(\"database\", \"people\").option(\"collection\", \"contacts\").save()\n",
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df = spark.read.format(\"mongodb\").load()\n",
    "df = spark.read.format(\"mongo\").load()\n",
    "df.printSchema()\n",
    "# df = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/people.contacts\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+-----+\n",
      "|                 _id|age| name|\n",
      "+--------------------+---+-----+\n",
      "|{63c6cc2814c2b47f...|158|Gloin|\n",
      "+--------------------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = \"{'$match': {'name': 'Gloin'}}\"\n",
    "df = spark.read.format(\"mongo\").option(\"pipeline\", pipeline).load()\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- crawlID: string (nullable = true)\n",
      " |-- currentURL: string (nullable = true)\n",
      " |-- enseigneID: string (nullable = true)\n",
      " |-- enseigneName: string (nullable = true)\n",
      " |-- job_id: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- other: string (nullable = true)\n",
      " |-- productBrand: string (nullable = true)\n",
      " |-- productCategory: string (nullable = true)\n",
      " |-- productDate: string (nullable = true)\n",
      " |-- productEAN: string (nullable = true)\n",
      " |-- productID: string (nullable = true)\n",
      " |-- productImage: string (nullable = true)\n",
      " |-- productIsAvailable: long (nullable = true)\n",
      " |-- productIsBio: long (nullable = true)\n",
      " |-- productIsFreezer: long (nullable = true)\n",
      " |-- productIsFresh: long (nullable = true)\n",
      " |-- productIsNew: long (nullable = true)\n",
      " |-- productLinkDetail: string (nullable = true)\n",
      " |-- productName: string (nullable = true)\n",
      " |-- productPackaging: string (nullable = true)\n",
      " |-- productPosition: long (nullable = true)\n",
      " |-- productPrice: double (nullable = true)\n",
      " |-- productPriceBase: double (nullable = true)\n",
      " |-- productPricePrevious: double (nullable = true)\n",
      " |-- productPromotionText: string (nullable = true)\n",
      " |-- productQuantityRating: long (nullable = true)\n",
      " |-- productRating: string (nullable = true)\n",
      " |-- productUnit: string (nullable = true)\n",
      " |-- productValueUnit: double (nullable = true)\n",
      " |-- promotion: long (nullable = true)\n",
      " |-- shopID: string (nullable = true)\n",
      " |-- typeCrawler: string (nullable = true)\n",
      " |-- zip_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df = spark.read.format(\"mongo\").option(\"pipeline\", pipeline).load()\n",
    "df = spark.read.format(\"mongo\").load()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_id',\n",
       " 'country',\n",
       " 'crawlID',\n",
       " 'currentURL',\n",
       " 'enseigneID',\n",
       " 'enseigneName',\n",
       " 'job_id',\n",
       " 'language',\n",
       " 'other',\n",
       " 'productBrand',\n",
       " 'productCategory',\n",
       " 'productDate',\n",
       " 'productEAN',\n",
       " 'productID',\n",
       " 'productImage',\n",
       " 'productIsAvailable',\n",
       " 'productIsBio',\n",
       " 'productIsFreezer',\n",
       " 'productIsFresh',\n",
       " 'productIsNew',\n",
       " 'productLinkDetail',\n",
       " 'productName',\n",
       " 'productPackaging',\n",
       " 'productPosition',\n",
       " 'productPrice',\n",
       " 'productPriceBase',\n",
       " 'productPricePrevious',\n",
       " 'productPromotionText',\n",
       " 'productQuantityRating',\n",
       " 'productRating',\n",
       " 'productUnit',\n",
       " 'productValueUnit',\n",
       " 'promotion',\n",
       " 'shopID',\n",
       " 'typeCrawler',\n",
       " 'zip_code']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all columnnames in order\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd.sample(False, 0.1, 81)\n",
    "type(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = rdd2.toDF(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df2.select(\"productCategory\").distinct().show()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe faster as rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of out-of-stock products by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unavailableProductsDf = df.filter(df[productIsAvailable].isNull()).count()\n",
    "categories = df.select(\"productCategory\").distinct().show() # convert to list\n",
    "dates = df.select(\"productDate\").distinct().show()\n",
    "\n",
    "# count unavailable products by category and date\n",
    "for category in categories:\n",
    "    for date in dates:\n",
    "        filtered_df = df.filter((df.productCategory == category) & (df.productDate == date))\n",
    "        #available_count = filtered_df.filter(filtered_df.productIsAvailable == \"yes\").count()\n",
    "        unavailable_count = filtered_df.filter(filtered_df.productIsAvailable == \"no\").count()\n",
    "        # see if total is always the same\n",
    "        # include location?\n",
    "    # each category one color line, x-axis date, y-axis count of unavailable products\n",
    "    plt.plot(dates, unavailable_count, label = category)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of out-of-stock products by departments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#departments = df.select(\"zip_code\").map\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(df)\n",
    "rdd2 = rdd.map(lambda x: \"\".join(list(x[\"zip_code\"])[:1]))\n",
    "#df2 = rdd2.toDF([\"name\",\"gender\",\"new_salary\"]   )\n",
    "departments = df.select(\"zip_code\").distinct().show()\n",
    "\n",
    "for element in rdd2.collect():\n",
    "    print(element)\n",
    "for department in departments:\n",
    "    for date in dates:\n",
    "        # count avaiable and unavailable products\n",
    "        filtered_df = df.filter((df.zip_code == department) & (df.productDate == date))\n",
    "        #available_count = filtered_df.filter(filtered_df.productIsAvailable == \"yes\").count()\n",
    "        unavailable_count = filtered_df.filter(filtered_df.productIsAvailable == \"no\").count()\n",
    "        # see if total is always the same\n",
    "    # each category one color line, x-axis date, y-axis count of unavailable products\n",
    "    plt.plot(dates, unavailable_count, label = category)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution of prices over time and by department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for department in departments:\n",
    "    for date in dates:\n",
    "        filtered_df = df.filter((df.zip_code == department) & (df.productDate == date))\n",
    "        # average prices over department\n",
    "        prices = filtered_df.filter\n",
    "        df.groupBy(\"zip_code\").agg(F.mean('productPrice'), F.count('productPrice')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market share of customers (based on the brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market shares by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "jupytext_formats": "ipynb,py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
